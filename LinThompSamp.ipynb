{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Thompson Sampling with Linear Payoff\n",
    "In This module contains a class that implements Thompson Sampling with Linear\n",
    "Payoff. Thompson Sampling with linear payoff is a contexutal multi-armed bandit\n",
    "algorithm which assume the underlying relationship between rewards and contexts\n",
    "is linear. The sampling method is used to balance the exploration and\n",
    "exploitation. Please check the reference for more details.\n",
    "\"\"\"\n",
    "import logging\n",
    "import six\n",
    "from six.moves import zip\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from .bandit import BaseBandit\n",
    "from ..utils import get_random_state\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LinThompSamp(BaseBandit):\n",
    "    r\"\"\"Thompson sampling with linear payoff.\n",
    "    Parameters\n",
    "    ----------\n",
    "    history_storage : HistoryStorage object\n",
    "        The HistoryStorage object to store history context, actions and rewards.\n",
    "    model_storage : ModelStorage object\n",
    "        The ModelStorage object to store model parameters.\n",
    "    action_storage : ActionStorage object\n",
    "        The ActionStorage object to store actions.\n",
    "    recommendation_cls : class (default: None)\n",
    "        The class used to initiate the recommendations. If None, then use\n",
    "        default Recommendation class.\n",
    "    delta: float, 0 < delta < 1\n",
    "        With probability 1 - delta, LinThompSamp satisfies the theoretical\n",
    "        regret bound.\n",
    "    R: float, R >= 0\n",
    "        Assume that the residual  :math:`ri(t) - bi(t)^T \\hat{\\mu}`\n",
    "        is R-sub-gaussian. In this case, R^2 represents the variance for\n",
    "        residuals of the linear model :math:`bi(t)^T`.\n",
    "    epsilon: float, 0 < epsilon < 1\n",
    "        A  parameter  used  by  the  Thompson Sampling algorithm.\n",
    "        If the total trials T is known, we can choose epsilon = 1/ln(T).\n",
    "    random_state: {int, np.random.RandomState} (default: None)\n",
    "        If int, np.random.RandomState will used it as seed. If None, a random\n",
    "        seed will be used.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1]  Shipra Agrawal, and Navin Goyal. \"Thompson Sampling for Contextual\n",
    "            Bandits with Linear Payoffs.\" Advances in Neural Information\n",
    "            Processing Systems 24. 2011.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, history_storage, model_storage, action_storage,\n",
    "                 recommendation_cls=None, context_dimension=128, delta=0.5,\n",
    "                 R=0.01, epsilon=0.5, random_state=None):\n",
    "        super(LinThompSamp, self).__init__(history_storage, model_storage,\n",
    "                                           action_storage, recommendation_cls)\n",
    "        self.random_state = get_random_state(random_state)\n",
    "        self.context_dimension = context_dimension\n",
    "\n",
    "        # 0 < delta < 1\n",
    "        if not isinstance(delta, float):\n",
    "            raise ValueError(\"delta should be float\")\n",
    "        elif (delta < 0) or (delta >= 1):\n",
    "            raise ValueError(\"delta should be in (0, 1]\")\n",
    "        else:\n",
    "            self.delta = delta\n",
    "\n",
    "        # R > 0\n",
    "        if not isinstance(R, float):\n",
    "            raise ValueError(\"R should be float\")\n",
    "        elif R <= 0:\n",
    "            raise ValueError(\"R should be positive\")\n",
    "        else:\n",
    "            self.R = R  # pylint: disable=invalid-name\n",
    "\n",
    "        # 0 < epsilon < 1\n",
    "        if not isinstance(epsilon, float):\n",
    "            raise ValueError(\"epsilon should be float\")\n",
    "        elif (epsilon < 0) or (epsilon > 1):\n",
    "            raise ValueError(\"epsilon should be in (0, 1)\")\n",
    "        else:\n",
    "            self.epsilon = epsilon\n",
    "\n",
    "        # model initialization\n",
    "        B = np.identity(self.context_dimension)  # pylint: disable=invalid-name\n",
    "        mu_hat = np.zeros(shape=(self.context_dimension, 1))\n",
    "        f = np.zeros(shape=(self.context_dimension, 1))\n",
    "        self._model_storage.save_model({'B': B, 'mu_hat': mu_hat, 'f': f})\n",
    "\n",
    "    def _linthompsamp_score(self, context):\n",
    "        \"\"\"Thompson Sampling\"\"\"\n",
    "        action_ids = list(six.viewkeys(context))\n",
    "        context_array = np.asarray([context[action_id]\n",
    "                                    for action_id in action_ids])\n",
    "        model = self._model_storage.get_model()\n",
    "        B = model['B']  # pylint: disable=invalid-name\n",
    "        mu_hat = model['mu_hat']\n",
    "        v = self.R * np.sqrt(24 / self.epsilon\n",
    "                             * self.context_dimension\n",
    "                             * np.log(1 / self.delta))\n",
    "        mu_tilde = self.random_state.multivariate_normal(\n",
    "            mu_hat.flat, v**2 * np.linalg.inv(B))[..., np.newaxis]\n",
    "        estimated_reward_array = context_array.dot(mu_hat)\n",
    "        score_array = context_array.dot(mu_tilde)\n",
    "\n",
    "        estimated_reward_dict = {}\n",
    "        uncertainty_dict = {}\n",
    "        score_dict = {}\n",
    "        for action_id, estimated_reward, score in zip(\n",
    "                action_ids, estimated_reward_array, score_array):\n",
    "            estimated_reward_dict[action_id] = float(estimated_reward)\n",
    "            score_dict[action_id] = float(score)\n",
    "            uncertainty_dict[action_id] = float(score - estimated_reward)\n",
    "        return estimated_reward_dict, uncertainty_dict, score_dict\n",
    "\n",
    "    def get_action(self, context, n_actions=None):\n",
    "        \"\"\"Return the action to perform\n",
    "        Parameters\n",
    "        ----------\n",
    "        context : dictionary\n",
    "            Contexts {action_id: context} of different actions.\n",
    "        n_actions: int (default: None)\n",
    "            Number of actions wanted to recommend users. If None, only return\n",
    "            one action. If -1, get all actions.\n",
    "        Returns\n",
    "        -------\n",
    "        history_id : int\n",
    "            The history id of the action.\n",
    "        recommendations : list of dict\n",
    "            Each dict contains\n",
    "            {Action object, estimated_reward, uncertainty}.\n",
    "        \"\"\"\n",
    "        if self._action_storage.count() == 0:\n",
    "            return self._get_action_with_empty_action_storage(context,\n",
    "                                                              n_actions)\n",
    "\n",
    "        if not isinstance(context, dict):\n",
    "            raise ValueError(\n",
    "                \"LinThompSamp requires context dict for all actions!\")\n",
    "        if n_actions == -1:\n",
    "            n_actions = self._action_storage.count()\n",
    "\n",
    "        estimated_reward, uncertainty, score = self._linthompsamp_score(context)\n",
    "\n",
    "        if n_actions is None:\n",
    "            recommendation_id = max(score, key=score.get)\n",
    "            recommendations = self._recommendation_cls(\n",
    "                action=self._action_storage.get(recommendation_id),\n",
    "                estimated_reward=estimated_reward[recommendation_id],\n",
    "                uncertainty=uncertainty[recommendation_id],\n",
    "                score=score[recommendation_id],\n",
    "            )\n",
    "        else:\n",
    "            recommendation_ids = sorted(score, key=score.get,\n",
    "                                        reverse=True)[:n_actions]\n",
    "            recommendations = []  # pylint: disable=redefined-variable-type\n",
    "            for action_id in recommendation_ids:\n",
    "                recommendations.append(self._recommendation_cls(\n",
    "                    action=self._action_storage.get(action_id),\n",
    "                    estimated_reward=estimated_reward[action_id],\n",
    "                    uncertainty=uncertainty[action_id],\n",
    "                    score=score[action_id],\n",
    "                ))\n",
    "\n",
    "        history_id = self._history_storage.add_history(context, recommendations)\n",
    "        return history_id, recommendations\n",
    "\n",
    "    def reward(self, history_id, rewards):\n",
    "        \"\"\"Reward the previous action with reward.\n",
    "        Parameters\n",
    "        ----------\n",
    "        history_id : int\n",
    "            The history id of the action to reward.\n",
    "        rewards : dictionary\n",
    "            The dictionary {action_id, reward}, where reward is a float.\n",
    "        \"\"\"\n",
    "        context = (self._history_storage\n",
    "                   .get_unrewarded_history(history_id)\n",
    "                   .context)\n",
    "\n",
    "        # Update the model\n",
    "        model = self._model_storage.get_model()\n",
    "        B = model['B']  # pylint: disable=invalid-name\n",
    "        f = model['f']\n",
    "\n",
    "        for action_id, reward in six.viewitems(rewards):\n",
    "            context_t = np.reshape(context[action_id], (-1, 1))\n",
    "            B += context_t.dot(context_t.T)  # pylint: disable=invalid-name\n",
    "            f += reward * context_t\n",
    "            mu_hat = np.linalg.inv(B).dot(f)\n",
    "        self._model_storage.save_model({'B': B, 'mu_hat': mu_hat, 'f': f})\n",
    "\n",
    "        # Update the history\n",
    "        self._history_storage.add_reward(history_id, rewards)\n",
    "\n",
    "    def add_action(self, actions):\n",
    "        \"\"\" Add new actions (if needed).\n",
    "        Parameters\n",
    "        ----------\n",
    "        actions : iterable\n",
    "            A list of Action oBjects for recommendation\n",
    "        \"\"\"\n",
    "        self._action_storage.add(actions)\n",
    "\n",
    "    def remove_action(self, action_id):\n",
    "        \"\"\"Remove action by id.\n",
    "        Parameters\n",
    "        ----------\n",
    "        action_id : int\n",
    "            The id of the action to remove.\n",
    "        \"\"\"\n",
    "        self._action_storage.remove(action_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
